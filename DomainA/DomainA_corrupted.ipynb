{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "torch.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataloader(\n",
    "#     batch_size: int = 128,\n",
    "#     num_samples: int = 100,\n",
    "#     spread: float = 0.5,\n",
    "#     split: float = 0.8,\n",
    "#     seed: int = 1234,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Creates the dataset\n",
    "#     :param batch_size: the batch size\n",
    "#     :param num_samples: number of items per class/label/letter\n",
    "#     :param spread: the std for normal sampling\n",
    "#     :param split: train-val split (<1). Number given is allotted as the train %\n",
    "#     :param seed: seed for the \"random\" dataset generator\n",
    "#     :return: the dataloaders\n",
    "#     \"\"\"\n",
    "\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     ideal_sensory_values = np.random.randint(-24, 23, (24, 16))\n",
    "#     dataset = list()\n",
    "#     classes = 24\n",
    "#     for letter in range(classes):\n",
    "#         for _ in range(num_samples):\n",
    "#             sensors = []\n",
    "\n",
    "#             for sensor in ideal_sensory_values[letter]:\n",
    "#                 sensors.append(np.random.normal(loc=sensor, scale=spread))\n",
    "            \n",
    "#             sensors= np.array(sensors)\n",
    "#             if np.random.choice([True,False],p=[0.7,0.3]):\n",
    "#                 indices = np.random.choice(np.arange(sensors.size), replace=False, size = int(sensors.size * 0.3))\n",
    "#                 sensors[indices] = np.random.choice([0,100])               \n",
    "#             dataset.append([sensors, np.array([letter])])\n",
    "\n",
    "#     x = list()\n",
    "#     y = list()\n",
    "\n",
    "#     for i in range(num_samples * 24): #24 if we train fully instead of classes\n",
    "#             x.append(dataset[i][0])\n",
    "#             y.append(dataset[i][1])\n",
    "\n",
    "#     tensor_x = torch.Tensor(x)\n",
    "#     tensor_y = torch.Tensor(y)\n",
    "\n",
    "#     # train_split = int(split * len(x))\n",
    "#     # val_split = len(x) - train_split\n",
    "#     train_split = int(split * len(x))\n",
    "#     val_split = len(x) - train_split\n",
    "\n",
    "#     tensor_dataset = TensorDataset(tensor_x, tensor_y)\n",
    "#     train_dataset, val_dataset = random_split(tensor_dataset, [train_split, val_split], generator=torch.Generator().manual_seed(1234))\n",
    "#     # train_dataset = tensor_dataset[:train_split] \n",
    "#     # val_dataset   = tensor_dataset[train_split:] \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#     return train_loader, val_loader, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(\n",
    "    batch_size: int = 128,\n",
    "    num_samples: int = 10000,\n",
    "    spread: float = 0.5,\n",
    "    split: float = 0.8,\n",
    "    seed: int = 1234,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates the dataset\n",
    "    :param batch_size: the batch size\n",
    "    :param num_samples: number of items per class/label/letter\n",
    "    :param spread: the std for normal sampling\n",
    "    :param split: train-val split (<1). Number given is allotted as the train %\n",
    "    :param seed: seed for the \"random\" dataset generator\n",
    "    :return: the dataloaders\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    ideal_sensory_values = np.random.randint(-24, 23, (24, 16))\n",
    "    \n",
    "\n",
    "    classes = 24\n",
    "\n",
    "    dataset     = list()\n",
    "    dataset_cor = list()\n",
    "\n",
    "    for letter in range(classes):\n",
    "        for _ in range(num_samples):\n",
    "            sensors = []\n",
    "            un_cor = []\n",
    "            for sensor in ideal_sensory_values[letter]:\n",
    "                r = np.random.normal(loc=sensor, scale=spread)\n",
    "                sensors.append(r)\n",
    "                un_cor.append(r)\n",
    "            \n",
    "            sensors = np.array(sensors)\n",
    "            un_cor = np.array(un_cor)\n",
    "            dataset.append([un_cor, np.array([letter])])\n",
    "    \n",
    "\n",
    "            if np.random.choice([True,False],p=[0.7,0.3]):\n",
    "                indices = np.random.choice(np.arange(sensors.size), replace=False, size=int(sensors.size * 0.3))\n",
    "                sensors[indices] = np.random.choice([-100,100])\n",
    "                \n",
    "            dataset_cor.append([sensors, np.array([letter])])\n",
    "            \n",
    "    \n",
    "    x = list()\n",
    "    w = list()\n",
    "    y = list()\n",
    "    \n",
    "    for i in range(num_samples * classes): #24 if we train fully instead of classes\n",
    "        x.append(dataset_cor[i][0])\n",
    "        w.append(dataset[i][0])\n",
    "        y.append(dataset_cor[i][1])\n",
    "\n",
    "    x = np.array(x)\n",
    "    w = np.array(w)\n",
    "    y = np.array(y)\n",
    "\n",
    "    tensor_x = torch.Tensor(x)\n",
    "    tensor_w = torch.Tensor(w)\n",
    "    tensor_y = torch.Tensor(y)\n",
    "\n",
    "    train_split = int(split * len(x))\n",
    "    val_split = len(x) - train_split\n",
    "\n",
    "    tensor_dataset = TensorDataset(tensor_x, tensor_w, tensor_y)\n",
    "\n",
    "    train_dataset, val_dataset = random_split(tensor_dataset, [train_split, val_split])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, x, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAE(nn.Module):\n",
    "    def __init__(self, in_size=16, latent_size=64):\n",
    "        super(LinearAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,latent_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, in_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_and_max(dataloader, device) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Finds the min and max of the dataset.\n",
    "    This is used for Normalization of the dataset.\n",
    "\n",
    "    :param dataloader: dataloader to calculate it for\n",
    "    :param device: device to run computations on\n",
    "    :return: tuple of mean and std\n",
    "    \"\"\"\n",
    "    min_val, max_val = torch.Tensor([999]).to(device), torch.Tensor([-999]).to(device)\n",
    "    for data, data_uncor, _ in tqdm(dataloader):\n",
    "        data_uncor = data_uncor.to(device)\n",
    "        min_val = torch.min(min_val, torch.min(data_uncor))\n",
    "        max_val = torch.max(max_val, torch.max(data_uncor))\n",
    "\n",
    "    return min_val.item(), max_val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1234)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearAE(16, 256).cuda()\n",
    "train_loader, val_loader, corrupted_dataset, uncor_dataset = prepare_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([100.        ,  14.35540801, -12.02123764, 100.        ,\n",
       "         -9.33854858,  -0.14932535,  17.25901441,   1.70367204,\n",
       "          6.74142256,  19.11366117,   5.54541005,  20.10880644,\n",
       "          2.13634056, 100.        , -19.6863874 , 100.        ]),\n",
       " array([ -4.96206429,  14.35540801, -12.02123764,  -0.63251244,\n",
       "         -9.33854858,  -0.14932535,  17.25901441,   1.70367204,\n",
       "          6.74142256,  19.11366117,   5.54541005,  20.10880644,\n",
       "          2.13634056,   3.98658565, -19.6863874 ,  -7.44503909]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrupted_dataset[0], uncor_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:03<00:00, 439.20it/s]\n"
     ]
    }
   ],
   "source": [
    "min_val, max_val = get_min_and_max(train_loader, device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 || Train Loss: 0.00858986299764365017 \n",
      "Epoch: 001 || Train Loss: 0.00829912059567868704 \n",
      "Epoch: 002 || Train Loss: 0.00802067039037744275 \n",
      "Epoch: 003 || Train Loss: 0.00777044168487191211 \n",
      "Epoch: 004 || Train Loss: 0.00756072256310532514 \n",
      "Epoch: 005 || Train Loss: 0.00737407410796731694 \n",
      "Epoch: 006 || Train Loss: 0.00718174670419345282 \n",
      "Epoch: 007 || Train Loss: 0.00702525631555666556 \n",
      "Epoch: 008 || Train Loss: 0.00689436893754949171 \n",
      "Epoch: 009 || Train Loss: 0.00676313549963136491 \n",
      "Epoch: 010 || Train Loss: 0.00663972909531245631 \n",
      "Epoch: 011 || Train Loss: 0.00653147411222259212 \n",
      "Epoch: 012 || Train Loss: 0.00644933758769184341 \n",
      "Epoch: 013 || Train Loss: 0.00634075252432376134 \n",
      "Epoch: 014 || Train Loss: 0.00623630272503942259 \n",
      "Epoch: 015 || Train Loss: 0.00604097409080713976 \n",
      "Epoch: 016 || Train Loss: 0.00587463004856059959 \n",
      "Epoch: 017 || Train Loss: 0.00575796936918050034 \n",
      "Epoch: 018 || Train Loss: 0.00563345705935110620 \n",
      "Epoch: 019 || Train Loss: 0.00554637003752092485 \n",
      "Epoch: 020 || Train Loss: 0.00544409622577950343 \n",
      "Epoch: 021 || Train Loss: 0.00536106260415787541 \n",
      "Epoch: 022 || Train Loss: 0.00529116430412977963 \n",
      "Epoch: 023 || Train Loss: 0.00521359360745797529 \n",
      "Epoch: 024 || Train Loss: 0.00514336452540010199 \n",
      "Epoch: 025 || Train Loss: 0.00507159231168528412 \n",
      "Epoch: 026 || Train Loss: 0.00500461488915607353 \n",
      "Epoch: 027 || Train Loss: 0.00491415060063203190 \n",
      "Epoch: 028 || Train Loss: 0.00482744508531565437 \n",
      "Epoch: 029 || Train Loss: 0.00474762777689223450 \n",
      "Epoch: 030 || Train Loss: 0.00468753742085148851 \n",
      "Epoch: 031 || Train Loss: 0.00463175649909923492 \n",
      "Epoch: 032 || Train Loss: 0.00457053327669079083 \n",
      "Epoch: 033 || Train Loss: 0.00454566802581151362 \n",
      "Epoch: 034 || Train Loss: 0.00451091816943759681 \n",
      "Epoch: 035 || Train Loss: 0.00445930957980453929 \n",
      "Epoch: 036 || Train Loss: 0.00442582771430412910 \n",
      "Epoch: 037 || Train Loss: 0.00440214715463419729 \n",
      "Epoch: 038 || Train Loss: 0.00437033557612448963 \n",
      "Epoch: 039 || Train Loss: 0.00434914089180529154 \n",
      "Epoch: 040 || Train Loss: 0.00431018144652868269 \n",
      "Epoch: 041 || Train Loss: 0.00427922601640845328 \n",
      "Epoch: 042 || Train Loss: 0.00427050129200021427 \n",
      "Epoch: 043 || Train Loss: 0.00423704483561838669 \n",
      "Epoch: 044 || Train Loss: 0.00420970655962203937 \n",
      "Epoch: 045 || Train Loss: 0.00418485766773422571 \n",
      "Epoch: 046 || Train Loss: 0.00416056686375911033 \n",
      "Epoch: 047 || Train Loss: 0.00412877349508926263 \n",
      "Epoch: 048 || Train Loss: 0.00407609485027690763 \n",
      "Epoch: 049 || Train Loss: 0.00401788067041585883 \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "  train_loss = 0\n",
    " \n",
    "  for batch_idx, data in enumerate(train_loader):\n",
    "      vector, vector_uncor, _ = data\n",
    "\n",
    "      vector     = vector.to(device)\n",
    "      vector     = (vector - min_val) / (max_val - min_val )\n",
    "      vector_uncor = vector_uncor.to(device)\n",
    "      vector_uncor = (vector_uncor - min_val) / (max_val - min_val )\n",
    "      \n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      y = model(vector)\n",
    "      # recon_batch, mu, logvar = model(x)\n",
    "      loss = F.mse_loss(y, vector_uncor)\n",
    "      # loss = loss_function(recon_batch, x, mu, logvar)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.item()\n",
    "\n",
    "  scheduler.step(train_loss/len(train_loader))\n",
    "  print(\"Epoch: {:03d} || Train Loss: {:.20f} \".format(epoch, train_loss / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "model_filename = 'domainA-encoder-corrupted.pt'\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "torch.save(model.state_dict(), model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 , 120) \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def DNN():\n",
    "  net = Net()\n",
    "  return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN().cuda()\n",
    "model.load_state_dict(torch.load('/home/ubuntu/Latent-Transfer/Validation-Model/models/dnn.pt'))\n",
    "\n",
    "enc_model = LinearAE(16,256).cuda()\n",
    "enc_model.load_state_dict(torch.load('/home/ubuntu/Latent-Transfer/DomainA/models/domainA-encoder-corrupted.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sensor Data (Unnormalized) -> \n",
      " tensor([-100.0000, -100.0000,   18.1098,    8.8315,  -18.9407,   -0.4759,\n",
      "          21.3509,   -7.9034,  -23.7517,  -23.0033, -100.0000,   20.9954,\n",
      "        -100.0000,    5.8936,   -9.8196,   20.2412], device='cuda:0')\n",
      "Input Sensor Data (Normalized)   -> \n",
      " tensor([-1.4737, -1.4737,  0.8811,  0.6961,  0.1424,  0.5105,  0.9457,  0.3624,\n",
      "         0.0465,  0.0614, -1.4737,  0.9386, -1.4737,  0.6375,  0.3242,  0.9235],\n",
      "       device='cuda:0')\n",
      "-----------------------------------------------------------\n",
      "Input Sensor Data (Normalized) -> \n",
      " tensor([-1.4737, -1.4737,  0.8811,  0.6961,  0.1424,  0.5105,  0.9457,  0.3624,\n",
      "         0.0465,  0.0614, -1.4737,  0.9386, -1.4737,  0.6375,  0.3242,  0.9235],\n",
      "       device='cuda:0')\n",
      "Reconstructed Sensor Data (Normalized) -> \n",
      " tensor([0.7940, 0.2542, 0.9102, 0.7272, 0.0686, 0.4870, 0.9371, 0.2764, 0.0117,\n",
      "        0.0465, 0.0963, 0.8993, 0.3245, 0.6179, 0.3526, 0.9829],\n",
      "       device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "-----------------------------------------------------------\n",
      "Input Uncorrupted data (Unnormalized)   -> \n",
      " tensor([ 13.1506, -15.9812,  18.1098,   8.8315, -18.9407,  -0.4759,  21.3509,\n",
      "         -7.9034, -23.7517, -23.0033, -22.2077,  20.9954, -10.5159,   5.8936,\n",
      "         -9.8196,  20.2412], device='cuda:0')\n",
      "Input Sensor Data (Unnormalized)        -> \n",
      " tensor([-100.0000, -100.0000,   18.1098,    8.8315,  -18.9407,   -0.4759,\n",
      "          21.3509,   -7.9034,  -23.7517,  -23.0033, -100.0000,   20.9954,\n",
      "        -100.0000,    5.8936,   -9.8196,   20.2412], device='cuda:0')\n",
      "Reconstruced Sensor Data (Unnormalized) -> \n",
      " tensor([ 13.7437, -13.3302,  19.5706,  10.3936, -22.6401,  -1.6555,  20.9228,\n",
      "        -12.2209, -25.4955, -23.7519, -21.2508,  19.0234,  -9.8080,   4.9108,\n",
      "         -8.3971,  23.2186], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test = val_loader.dataset[0][0].cuda()\n",
    "test_crct = val_loader.dataset[0][1].cuda()\n",
    "inp = (test - min_val) / (max_val - min_val)\n",
    "enc = enc_model.encoder(inp)\n",
    "recon = enc_model.decoder(enc)\n",
    "reconstructed = (recon  *  (max_val - min_val)) + min_val\n",
    "print('Input Sensor Data (Unnormalized) -> \\n',test)\n",
    "print('Input Sensor Data (Normalized)   -> \\n',inp)\n",
    "print('-----------------------------------------------------------')\n",
    "print('Input Sensor Data (Normalized) -> \\n',inp)\n",
    "print('Reconstructed Sensor Data (Normalized) -> \\n',recon)\n",
    "print('-----------------------------------------------------------')\n",
    "print('Input Uncorrupted data (Unnormalized)      -> \\n',test_crct )\n",
    "print('Input Corrupted Sensor Data (Unnormalized) -> \\n',test) \n",
    "print('Reconstruced Sensor Data (Unnormalized)    -> \\n',reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparative Accuracy        : 38.13333333333333\n",
      "True Label Accuracy         : 38.13333333333333\n",
      "Reconstructed Label Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "p = 0\n",
    "r = 0\n",
    "\n",
    "for i in range(0, len(val_loader)):\n",
    "\n",
    "    test = val_loader.dataset[i][0].cuda()\n",
    "    test_label = val_loader.dataset[i][2].cuda()\n",
    "    inp = (test - min_val) / (max_val - min_val)\n",
    "\n",
    "# print('-------------------------------------------------------------------')\n",
    "# print('Input Sensor Data (Unnormalized) -> \\n',test)\n",
    "# print('Input Sensor Data (Normalized)   -> \\n',inp)\n",
    "\n",
    "    \n",
    "    enc = enc_model.encoder(inp)\n",
    "    recon = enc_model.decoder(enc)\n",
    "# print('-------------------------------------------------------------------')\n",
    "# print('Input Sensor Data (Normalized) -> \\n',inp)\n",
    "# print('Input Sensor Data (Normalized) -> \\n',recon)\n",
    "    reconstructed = (recon  *  (max_val - min_val)) + min_val\n",
    "# print('-------------------------------------------------------------------')\n",
    "# print('Input Sensor Data (Unnormalized)        -> \\n',test) \n",
    "# print('Reconstruced Sensor Data (Unnormalized) -> \\n',reconstructed)\n",
    "# print('-------------------------------------------------------------------')\n",
    "    x_check = torch.stack([test ,reconstructed])\n",
    "    out = model(x_check)\n",
    "    _, preds = torch.max(out, 1)\n",
    "    # print(preds[0], preds[1], test_label)\n",
    "    if (preds[0]==preds[1]):\n",
    "        s = s + 1\n",
    "    if (preds[0]==test_label):\n",
    "        p = p + 1\n",
    "    if (preds[1]==test_label):\n",
    "        r = r + 1\n",
    "\n",
    "\n",
    "print('Comparative Accuracy        : {}'.format((s / len(val_loader)) * 100))   \n",
    "print('True Label Accuracy         : {}'.format((p / len(val_loader)) * 100))\n",
    "print('Reconstructed Label Accuracy: {}'.format((r / len(val_loader)) * 100))\n",
    "# print('Prediction of the Input data         : ',preds[0])\n",
    "# print('Prediction of the Reconstructed data : ',preds[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08089c1e34da0e306e70ecc592d166e715d3323c59c0e51c1fac58ee58ba508c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('latent')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
